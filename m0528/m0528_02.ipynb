{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "(train_input, train_target), (test_input,test_target) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.00392157, 0.        , 0.        , 0.02745098,\n",
       "       0.        , 0.14509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.00784314, 0.        ,\n",
       "       0.10588235, 0.32941176, 0.04313725, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.46666667,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.        , 0.        , 0.34509804, 0.56078431,\n",
       "       0.43137255, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.08627451, 0.36470588, 0.41568627, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01568627, 0.        ,\n",
       "       0.20784314, 0.50588235, 0.47058824, 0.57647059, 0.68627451,\n",
       "       0.61568627, 0.65098039, 0.52941176, 0.60392157, 0.65882353,\n",
       "       0.54901961, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.        , 0.04313725, 0.5372549 , 0.50980392,\n",
       "       0.50196078, 0.62745098, 0.69019608, 0.62352941, 0.65490196,\n",
       "       0.69803922, 0.58431373, 0.59215686, 0.56470588, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.        , 0.00784314,\n",
       "       0.00392157, 0.        , 0.01176471, 0.        , 0.        ,\n",
       "       0.45098039, 0.44705882, 0.41568627, 0.5372549 , 0.65882353,\n",
       "       0.6       , 0.61176471, 0.64705882, 0.65490196, 0.56078431,\n",
       "       0.61568627, 0.61960784, 0.04313725, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00392157, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
       "       0.        , 0.        , 0.34901961, 0.54509804, 0.35294118,\n",
       "       0.36862745, 0.6       , 0.58431373, 0.51372549, 0.59215686,\n",
       "       0.6627451 , 0.6745098 , 0.56078431, 0.62352941, 0.6627451 ,\n",
       "       0.18823529, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00784314, 0.01568627,\n",
       "       0.00392157, 0.        , 0.        , 0.        , 0.38431373,\n",
       "       0.53333333, 0.43137255, 0.42745098, 0.43137255, 0.63529412,\n",
       "       0.52941176, 0.56470588, 0.58431373, 0.62352941, 0.65490196,\n",
       "       0.56470588, 0.61960784, 0.6627451 , 0.46666667, 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.00784314, 0.00392157,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.10196078, 0.42352941, 0.45882353, 0.38823529, 0.43529412,\n",
       "       0.45882353, 0.53333333, 0.61176471, 0.5254902 , 0.60392157,\n",
       "       0.60392157, 0.61176471, 0.62745098, 0.55294118, 0.57647059,\n",
       "       0.61176471, 0.69803922, 0.        , 0.01176471, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.08235294, 0.20784314, 0.36078431, 0.45882353, 0.43529412,\n",
       "       0.40392157, 0.45098039, 0.50588235, 0.5254902 , 0.56078431,\n",
       "       0.60392157, 0.64705882, 0.66666667, 0.60392157, 0.59215686,\n",
       "       0.60392157, 0.56078431, 0.54117647, 0.58823529, 0.64705882,\n",
       "       0.16862745, 0.        , 0.        , 0.09019608, 0.21176471,\n",
       "       0.25490196, 0.29803922, 0.33333333, 0.4627451 , 0.50196078,\n",
       "       0.48235294, 0.43529412, 0.44313725, 0.4627451 , 0.49803922,\n",
       "       0.49019608, 0.54509804, 0.52156863, 0.53333333, 0.62745098,\n",
       "       0.54901961, 0.60784314, 0.63137255, 0.56470588, 0.60784314,\n",
       "       0.6745098 , 0.63137255, 0.74117647, 0.24313725, 0.        ,\n",
       "       0.26666667, 0.36862745, 0.35294118, 0.43529412, 0.44705882,\n",
       "       0.43529412, 0.44705882, 0.45098039, 0.49803922, 0.52941176,\n",
       "       0.53333333, 0.56078431, 0.49411765, 0.49803922, 0.59215686,\n",
       "       0.60392157, 0.56078431, 0.58039216, 0.49019608, 0.63529412,\n",
       "       0.63529412, 0.56470588, 0.54117647, 0.6       , 0.63529412,\n",
       "       0.76862745, 0.22745098, 0.2745098 , 0.6627451 , 0.50588235,\n",
       "       0.40784314, 0.38431373, 0.39215686, 0.36862745, 0.38039216,\n",
       "       0.38431373, 0.4       , 0.42352941, 0.41568627, 0.46666667,\n",
       "       0.47058824, 0.50588235, 0.58431373, 0.61176471, 0.65490196,\n",
       "       0.74509804, 0.74509804, 0.76862745, 0.77647059, 0.77647059,\n",
       "       0.73333333, 0.77254902, 0.74117647, 0.72156863, 0.14117647,\n",
       "       0.0627451 , 0.49411765, 0.67058824, 0.7372549 , 0.7372549 ,\n",
       "       0.72156863, 0.67058824, 0.6       , 0.52941176, 0.47058824,\n",
       "       0.49411765, 0.49803922, 0.57254902, 0.7254902 , 0.76470588,\n",
       "       0.81960784, 0.81568627, 1.        , 0.81960784, 0.69411765,\n",
       "       0.96078431, 0.98823529, 0.98431373, 0.98431373, 0.96862745,\n",
       "       0.8627451 , 0.80784314, 0.19215686, 0.        , 0.        ,\n",
       "       0.        , 0.04705882, 0.2627451 , 0.41568627, 0.64313725,\n",
       "       0.7254902 , 0.78039216, 0.82352941, 0.82745098, 0.82352941,\n",
       "       0.81568627, 0.74509804, 0.58823529, 0.32156863, 0.03137255,\n",
       "       0.        , 0.        , 0.        , 0.69803922, 0.81568627,\n",
       "       0.7372549 , 0.68627451, 0.63529412, 0.61960784, 0.59215686,\n",
       "       0.04313725, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리\n",
    "\n",
    "train_input = train_input / 255\n",
    "train_scaled = train_input.reshape(-1,28*28)\n",
    "train_scaled[0]\n",
    "\n",
    "test_input = test_input / 255\n",
    "test_scaled = test_input.reshape(-1,28*28)\n",
    "test_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 - 인공신경망 모델\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 머신러닝 - cross_validate 검증실행\n",
    "# 딥러닝 - cross_calidate 검증안함 (시간이 너무 오래 걸림)\n",
    "\n",
    "# 딥러닝 - 훈련세트, 검증세트, 테스트세트(train,val,test)\n",
    "train_scaled,val_scaled,train_target,val_target = train_test_split(train_scaled,train_target,test_size=0.2,random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784) (12000, 784)\n",
      "(48000,) (12000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_scaled.shape,val_scaled.shape)\n",
    "print(train_target.shape,val_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝 훈련\n",
    "# 1. dense층\n",
    "# 결과의 클래스\n",
    "dense = keras.layers.Dense(10,activation='softmax',input_shape=(784,))  \n",
    "                # 10 : target 클래스\n",
    "                # softmax : 활성화함수\n",
    "                # input_shape : 입력된 데이터 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델결정\n",
    "model = keras.Sequential([dense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 모델설정부분\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "                # loss : 다중분류 결과값에 원핫인코딩을 사용하지 않으면 sparse_categorical_crossentropy\n",
    "                # loss : 다중분류 결과값에 원핫인코딩을 사용하면 categorical_crossentropy\n",
    "                # 이진분류 : 원핫인코딩을 사용하지 않으면 sparse_binary_crossentropy\n",
    "                # 이진분류 : 원핫인코딩을 사용하면 binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 441us/step - accuracy: 0.7457 - loss: 0.7647\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445us/step - accuracy: 0.8333 - loss: 0.4803\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 433us/step - accuracy: 0.8472 - loss: 0.4499\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 434us/step - accuracy: 0.8532 - loss: 0.4310\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431us/step - accuracy: 0.8535 - loss: 0.4314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x181e6cc6570>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 모델 훈련\n",
    "model.fit(train_scaled,train_target,epochs=5)\n",
    "                # epochs : 반복횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430us/step - accuracy: 0.8471 - loss: 0.4505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4414416551589966, 0.8510833382606506]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 모델 평가 - 손실률, 정확도\n",
    "model.evaluate(val_scaled,val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 층만들기 dense = keras.layers.Dense(10,activation='softmax',input_shape=(784,))  \n",
    "# 모델결정 model = keras.Sequential([dense])\n",
    "# 모델설정 model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "# 모델훈련 model.fit(train_scaled,train_target,epochs=5)\n",
    "# 모델평가 model.evaluate(val_scaled,val_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
